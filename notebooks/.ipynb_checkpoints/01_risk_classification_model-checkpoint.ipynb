{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Risk Classification Model\n",
    "\n",
    "## Objective\n",
    "Build a machine learning model to predict whether a stock will experience a significant drawdown (>10%) in the next 30 trading days. This replaces the manual weighted risk formula with a data-driven approach.\n",
    "\n",
    "## Approach\n",
    "1. **Data Collection** — 5 years of daily OHLCV data for 50 tech stocks from Yahoo Finance\n",
    "2. **Feature Engineering** — Technical indicators, volatility metrics, momentum signals\n",
    "3. **Target Variable** — Binary label: 1 if stock drops >10% in next 30 days, 0 otherwise\n",
    "4. **Model Training** — Compare Logistic Regression, Random Forest, XGBoost\n",
    "5. **Evaluation** — AUC-ROC, precision-recall, confusion matrix, calibration\n",
    "6. **Export** — Save best model for integration with the live application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. IMPORTS & SETUP\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, average_precision_score,\n",
    "    calibration_curve, f1_score, accuracy_score\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print('All imports successful.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Collection\n",
    "\n",
    "We fetch 5 years of daily OHLCV data for our tech stock universe from Yahoo Finance. We also download S&P 500 (SPY) for beta calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock universe (same as our platform)\n",
    "SYMBOLS = [\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'META', 'TSLA', 'NFLX',\n",
    "    'ADBE', 'CRM', 'ORCL', 'CSCO', 'INTC', 'AMD', 'QCOM', 'TXN',\n",
    "    'AVGO', 'INTU', 'AMAT', 'LRCX', 'MU', 'KLAC', 'SNPS', 'CDNS',\n",
    "    'MCHP', 'MRVL', 'NXPI', 'ADI', 'SWKS', 'QRVO', 'UBER', 'ABNB',\n",
    "    'SNOW', 'ZM', 'DOCU', 'SHOP', 'COIN', 'RBLX', 'DDOG', 'NET',\n",
    "    'CRWD', 'ZS', 'PANW', 'FTNT', 'OKTA', 'NOW', 'WDAY', 'TEAM',\n",
    "]\n",
    "\n",
    "# Add SPY for market benchmark (beta calculation)\n",
    "ALL_TICKERS = SYMBOLS + ['SPY']\n",
    "\n",
    "print(f'Downloading 5 years of data for {len(ALL_TICKERS)} tickers...')\n",
    "raw_data = yf.download(ALL_TICKERS, period='5y', interval='1d', group_by='ticker', auto_adjust=True)\n",
    "print(f'Download complete. Shape: {raw_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the multi-level column data into a clean long-format DataFrame\n",
    "records = []\n",
    "for symbol in ALL_TICKERS:\n",
    "    try:\n",
    "        df = raw_data[symbol].dropna(subset=['Close'])\n",
    "        df = df.reset_index()\n",
    "        df['symbol'] = symbol\n",
    "        df.columns = [c.lower() if c != 'symbol' else c for c in df.columns]\n",
    "        records.append(df[['symbol', 'date', 'open', 'high', 'low', 'close', 'volume']])\n",
    "    except Exception as e:\n",
    "        print(f'  Skipping {symbol}: {e}')\n",
    "\n",
    "data = pd.concat(records, ignore_index=True)\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data = data.sort_values(['symbol', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f'\\nTotal records: {len(data):,}')\n",
    "print(f'Date range: {data.date.min().date()} to {data.date.max().date()}')\n",
    "print(f'Stocks loaded: {data.symbol.nunique()}')\n",
    "print(f'\\nRecords per stock:')\n",
    "print(data.groupby('symbol').size().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data quality check\n",
    "missing = data.isnull().sum()\n",
    "print('Missing values:')\n",
    "print(missing[missing > 0] if missing.any() else 'None')\n",
    "\n",
    "# Check for stocks with insufficient data\n",
    "stock_counts = data.groupby('symbol').size()\n",
    "short_stocks = stock_counts[stock_counts < 500]\n",
    "if len(short_stocks) > 0:\n",
    "    print(f'\\nStocks with < 500 trading days (may have IPO within 5yr window):')\n",
    "    print(short_stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Engineering\n",
    "\n",
    "We compute technical features that capture different aspects of stock behavior:\n",
    "\n",
    "| Category | Features | Rationale |\n",
    "|----------|----------|-----------|\n",
    "| Volatility | 21d, 63d rolling std | Captures short and medium-term risk |\n",
    "| Momentum | 5d, 10d, 21d, 63d returns | Price trend signals |\n",
    "| RSI | 14-day RSI | Overbought/oversold indicator |\n",
    "| MACD | MACD line, signal, histogram | Trend strength |\n",
    "| Bollinger | Width, %B position | Volatility + mean reversion |\n",
    "| Volume | Volume ratio (vs 50d avg) | Unusual activity signal |\n",
    "| Drawdown | Max drawdown (trailing 63d) | Downside risk history |\n",
    "| Beta | Rolling 63d beta vs SPY | Systematic risk exposure |\n",
    "| Price | Distance from 52-week high/low | Relative valuation signal |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate SPY for beta calculation\n",
    "spy_data = data[data['symbol'] == 'SPY'][['date', 'close']].rename(columns={'close': 'spy_close'})\n",
    "spy_data['spy_return'] = spy_data['spy_close'].pct_change()\n",
    "\n",
    "# Remove SPY from main data\n",
    "stock_data = data[data['symbol'] != 'SPY'].copy()\n",
    "\n",
    "# Merge SPY returns for beta calculation\n",
    "stock_data = stock_data.merge(spy_data[['date', 'spy_return']], on='date', how='left')\n",
    "\n",
    "print(f'Stock data shape: {stock_data.shape}')\n",
    "print(f'SPY data shape: {spy_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(df):\n",
    "    \"\"\"\n",
    "    Compute all technical features for a single stock's time series.\n",
    "    Input: DataFrame with columns [date, open, high, low, close, volume, spy_return]\n",
    "    Output: DataFrame with all original + feature columns\n",
    "    \"\"\"\n",
    "    df = df.sort_values('date').copy()\n",
    "    close = df['close']\n",
    "    high = df['high']\n",
    "    low = df['low']\n",
    "    volume = df['volume']\n",
    "    \n",
    "    # --- Daily Returns ---\n",
    "    df['daily_return'] = close.pct_change()\n",
    "    \n",
    "    # --- Volatility ---\n",
    "    df['volatility_21d'] = df['daily_return'].rolling(21).std() * np.sqrt(252)  # annualized\n",
    "    df['volatility_63d'] = df['daily_return'].rolling(63).std() * np.sqrt(252)\n",
    "    \n",
    "    # --- Momentum / Returns ---\n",
    "    df['return_5d'] = close.pct_change(5)\n",
    "    df['return_10d'] = close.pct_change(10)\n",
    "    df['return_21d'] = close.pct_change(21)\n",
    "    df['return_63d'] = close.pct_change(63)\n",
    "    \n",
    "    # --- RSI (14-day) ---\n",
    "    delta = close.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    avg_gain = gain.rolling(14).mean()\n",
    "    avg_loss = loss.rolling(14).mean()\n",
    "    rs = avg_gain / (avg_loss + 1e-10)\n",
    "    df['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # --- MACD ---\n",
    "    ema_12 = close.ewm(span=12, adjust=False).mean()\n",
    "    ema_26 = close.ewm(span=26, adjust=False).mean()\n",
    "    df['macd_line'] = ema_12 - ema_26\n",
    "    df['macd_signal'] = df['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "    df['macd_histogram'] = df['macd_line'] - df['macd_signal']\n",
    "    \n",
    "    # --- Bollinger Bands (20-day, 2 std) ---\n",
    "    sma_20 = close.rolling(20).mean()\n",
    "    std_20 = close.rolling(20).std()\n",
    "    bb_upper = sma_20 + 2 * std_20\n",
    "    bb_lower = sma_20 - 2 * std_20\n",
    "    df['bb_width'] = (bb_upper - bb_lower) / (sma_20 + 1e-10)\n",
    "    df['bb_position'] = (close - bb_lower) / (bb_upper - bb_lower + 1e-10)  # %B\n",
    "    \n",
    "    # --- Volume Ratio ---\n",
    "    df['volume_ratio'] = volume / (volume.rolling(50).mean() + 1e-10)\n",
    "    \n",
    "    # --- Max Drawdown (trailing 63 days) ---\n",
    "    rolling_max = close.rolling(63, min_periods=1).max()\n",
    "    drawdown = (close - rolling_max) / (rolling_max + 1e-10)\n",
    "    df['max_drawdown_63d'] = drawdown.rolling(63, min_periods=1).min()\n",
    "    \n",
    "    # --- Beta vs SPY (rolling 63-day) ---\n",
    "    cov = df['daily_return'].rolling(63).cov(df['spy_return'])\n",
    "    var = df['spy_return'].rolling(63).var()\n",
    "    df['beta_63d'] = cov / (var + 1e-10)\n",
    "    \n",
    "    # --- Distance from 52-week high/low ---\n",
    "    high_252 = high.rolling(252, min_periods=63).max()\n",
    "    low_252 = low.rolling(252, min_periods=63).min()\n",
    "    df['dist_from_52w_high'] = (close - high_252) / (high_252 + 1e-10)\n",
    "    df['dist_from_52w_low'] = (close - low_252) / (low_252 + 1e-10)\n",
    "    \n",
    "    # --- Average True Range (14-day) ---\n",
    "    tr = pd.DataFrame({\n",
    "        'hl': high - low,\n",
    "        'hc': abs(high - close.shift(1)),\n",
    "        'lc': abs(low - close.shift(1))\n",
    "    }).max(axis=1)\n",
    "    df['atr_14'] = tr.rolling(14).mean() / (close + 1e-10)  # normalized\n",
    "    \n",
    "    return df\n",
    "\n",
    "print('Feature computation function defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute features for all stocks\n",
    "print('Computing features for each stock...')\n",
    "featured_dfs = []\n",
    "for symbol in stock_data['symbol'].unique():\n",
    "    sdf = stock_data[stock_data['symbol'] == symbol].copy()\n",
    "    sdf = compute_features(sdf)\n",
    "    featured_dfs.append(sdf)\n",
    "\n",
    "featured_data = pd.concat(featured_dfs, ignore_index=True)\n",
    "print(f'Featured data shape: {featured_data.shape}')\n",
    "print(f'\\nNew features added ({len(featured_data.columns) - len(stock_data.columns)}):')\n",
    "\n",
    "feature_cols = [\n",
    "    'volatility_21d', 'volatility_63d',\n",
    "    'return_5d', 'return_10d', 'return_21d', 'return_63d',\n",
    "    'rsi_14', 'macd_line', 'macd_signal', 'macd_histogram',\n",
    "    'bb_width', 'bb_position', 'volume_ratio',\n",
    "    'max_drawdown_63d', 'beta_63d',\n",
    "    'dist_from_52w_high', 'dist_from_52w_low', 'atr_14'\n",
    "]\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Target Variable Construction\n",
    "\n",
    "**Target:** Does the stock drop >10% at any point in the next 30 trading days?\n",
    "\n",
    "This is a forward-looking label — we can only construct it with hindsight, which is why we need historical data. We use the minimum close price in the next 30 days compared to today's close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRAWDOWN_THRESHOLD = -0.10  # 10% drop\n",
    "FORWARD_DAYS = 30\n",
    "\n",
    "def create_target(df):\n",
    "    \"\"\"\n",
    "    For each row, compute the max drawdown over the next 30 trading days.\n",
    "    Label = 1 if drawdown exceeds threshold (high risk), 0 otherwise.\n",
    "    \"\"\"\n",
    "    df = df.sort_values('date').copy()\n",
    "    close = df['close'].values\n",
    "    n = len(close)\n",
    "    \n",
    "    forward_min = np.full(n, np.nan)\n",
    "    for i in range(n - FORWARD_DAYS):\n",
    "        future_window = close[i+1 : i+1+FORWARD_DAYS]\n",
    "        forward_min[i] = np.min(future_window)\n",
    "    \n",
    "    df['forward_min_close'] = forward_min\n",
    "    df['forward_drawdown'] = (df['forward_min_close'] - df['close']) / df['close']\n",
    "    df['target'] = (df['forward_drawdown'] <= DRAWDOWN_THRESHOLD).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print('Creating target variable...')\n",
    "target_dfs = []\n",
    "for symbol in featured_data['symbol'].unique():\n",
    "    sdf = featured_data[featured_data['symbol'] == symbol].copy()\n",
    "    sdf = create_target(sdf)\n",
    "    target_dfs.append(sdf)\n",
    "\n",
    "model_data = pd.concat(target_dfs, ignore_index=True)\n",
    "\n",
    "# Drop rows where target can't be computed (last 30 days) or features have NaN\n",
    "model_data = model_data.dropna(subset=['target'] + feature_cols)\n",
    "\n",
    "print(f'\\nFinal dataset shape: {model_data.shape}')\n",
    "print(f'Date range: {model_data.date.min().date()} to {model_data.date.max().date()}')\n",
    "print(f'\\nTarget distribution:')\n",
    "print(model_data['target'].value_counts())\n",
    "print(f'\\nPositive rate (high risk days): {model_data.target.mean():.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution over time\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Monthly positive rate\n",
    "monthly = model_data.set_index('date').groupby(pd.Grouper(freq='M'))['target'].mean()\n",
    "axes[0].bar(monthly.index, monthly.values, width=20, color='indianred', alpha=0.7)\n",
    "axes[0].set_title('Monthly Proportion of High-Risk Labels', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Positive Rate')\n",
    "axes[0].axhline(y=model_data.target.mean(), color='black', linestyle='--', alpha=0.5, label=f'Overall: {model_data.target.mean():.2%}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Per-stock positive rate\n",
    "stock_rates = model_data.groupby('symbol')['target'].mean().sort_values(ascending=False)\n",
    "axes[1].barh(stock_rates.index, stock_rates.values, color='steelblue', alpha=0.7)\n",
    "axes[1].set_xlabel('Positive Rate (% of days labeled high risk)')\n",
    "axes[1].set_title('High-Risk Label Rate by Stock', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(x=model_data.target.mean(), color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('target_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature statistics\n",
    "print('Feature Statistics:')\n",
    "print('=' * 60)\n",
    "model_data[feature_cols].describe().T[['mean', 'std', 'min', '50%', 'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "corr = model_data[feature_cols + ['target']].corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, vmin=-1, vmax=1, ax=ax, square=True)\n",
    "ax.set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Top correlations with target\n",
    "print('\\nCorrelation with target (high risk):')\n",
    "target_corr = corr['target'].drop('target').abs().sort_values(ascending=False)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions: High Risk vs Normal\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "top_features = target_corr.head(9).index.tolist()\n",
    "\n",
    "for i, feat in enumerate(top_features):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    for label, color, name in [(0, 'steelblue', 'Normal'), (1, 'indianred', 'High Risk')]:\n",
    "        subset = model_data[model_data['target'] == label][feat].dropna()\n",
    "        ax.hist(subset, bins=50, alpha=0.5, color=color, label=name, density=True)\n",
    "    ax.set_title(feat, fontweight='bold')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Feature Distributions: Normal vs High Risk', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Train-Test Split (Time-Series Aware)\n",
    "\n",
    "**Critical:** We use a temporal split to avoid data leakage. The model trains on older data and tests on more recent data — just like in production.\n",
    "\n",
    "- **Train:** 2021-01 to 2024-12\n",
    "- **Test:** 2025-01 onwards\n",
    "\n",
    "We also use TimeSeriesSplit for cross-validation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal train-test split\n",
    "SPLIT_DATE = '2025-01-01'\n",
    "\n",
    "train = model_data[model_data['date'] < SPLIT_DATE].copy()\n",
    "test = model_data[model_data['date'] >= SPLIT_DATE].copy()\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "y_train = train['target']\n",
    "X_test = test[feature_cols]\n",
    "y_test = test['target']\n",
    "\n",
    "print(f'Train: {len(train):,} samples ({train.date.min().date()} to {train.date.max().date()})')\n",
    "print(f'Test:  {len(test):,} samples ({test.date.min().date()} to {test.date.max().date()})')\n",
    "print(f'\\nTrain positive rate: {y_train.mean():.2%}')\n",
    "print(f'Test positive rate:  {y_test.mean():.2%}')\n",
    "print(f'\\nFeatures: {len(feature_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (important for Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('Features scaled with StandardScaler.')\n",
    "print(f'Train shape: {X_train_scaled.shape}')\n",
    "print(f'Test shape:  {X_test_scaled.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Model Training & Comparison\n",
    "\n",
    "We train three models:\n",
    "1. **Logistic Regression** — Simple, interpretable baseline\n",
    "2. **Random Forest** — Ensemble, handles non-linearities\n",
    "3. **XGBoost** — Gradient boosting, typically best performance on tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        C=1.0, class_weight='balanced', max_iter=1000, random_state=42\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=300, max_depth=10, min_samples_leaf=20,\n",
    "        class_weight='balanced', random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "        scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),\n",
    "        min_child_weight=10, subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=42, eval_metric='logloss', verbosity=0\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Train all models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f'\\nTraining {name}...')\n",
    "    \n",
    "    # Use scaled data for LogReg, raw for tree-based\n",
    "    if 'Logistic' in name:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    ap = average_precision_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'y_pred': y_pred,\n",
    "        'auc_roc': auc,\n",
    "        'avg_precision': ap,\n",
    "        'f1': f1,\n",
    "        'accuracy': acc,\n",
    "    }\n",
    "    \n",
    "    print(f'  AUC-ROC: {auc:.4f} | Avg Precision: {ap:.4f} | F1: {f1:.4f} | Accuracy: {acc:.4f}')\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('MODEL COMPARISON SUMMARY')\n",
    "print('=' * 60)\n",
    "summary = pd.DataFrame({name: {k: v for k, v in r.items() if k not in ['model', 'y_pred_proba', 'y_pred']} \n",
    "                         for name, r in results.items()}).T\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# --- ROC Curve ---\n",
    "ax = axes[0]\n",
    "for name, r in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, r['y_pred_proba'])\n",
    "    ax.plot(fpr, tpr, label=f\"{name} (AUC={r['auc_roc']:.3f})\", linewidth=2)\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# --- Precision-Recall Curve ---\n",
    "ax = axes[1]\n",
    "for name, r in results.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_test, r['y_pred_proba'])\n",
    "    ax.plot(recall, precision, label=f\"{name} (AP={r['avg_precision']:.3f})\", linewidth=2)\n",
    "ax.axhline(y=y_test.mean(), color='k', linestyle='--', alpha=0.3, label=f'Baseline ({y_test.mean():.3f})')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision-Recall Curve', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# --- Calibration Curve ---\n",
    "ax = axes[2]\n",
    "for name, r in results.items():\n",
    "    prob_true, prob_pred = calibration_curve(y_test, r['y_pred_proba'], n_bins=10)\n",
    "    ax.plot(prob_pred, prob_true, 's-', label=name, linewidth=2)\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Perfect calibration')\n",
    "ax.set_xlabel('Mean Predicted Probability')\n",
    "ax.set_ylabel('Fraction of Positives')\n",
    "ax.set_title('Calibration Curve', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle('Model Evaluation', fontsize=16, fontweight='bold', y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (name, r) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_test, r['y_pred'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['Normal', 'High Risk'],\n",
    "                yticklabels=['Normal', 'High Risk'])\n",
    "    axes[idx].set_title(f'{name}', fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "\n",
    "plt.suptitle('Confusion Matrices', fontsize=16, fontweight='bold', y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification reports\n",
    "for name, r in results.items():\n",
    "    print(f'\\n{\"=\" * 50}')\n",
    "    print(f'{name} — Classification Report')\n",
    "    print(f'{\"=\" * 50}')\n",
    "    print(classification_report(y_test, r['y_pred'], target_names=['Normal', 'High Risk']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost feature importance (gain-based)\n",
    "best_model_name = max(results, key=lambda k: results[k]['auc_roc'])\n",
    "print(f'Best model by AUC-ROC: {best_model_name} ({results[best_model_name][\"auc_roc\"]:.4f})')\n",
    "\n",
    "# XGBoost importance\n",
    "xgb_model = results['XGBoost']['model']\n",
    "xgb_importance = pd.Series(xgb_model.feature_importances_, index=feature_cols).sort_values(ascending=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# XGBoost\n",
    "xgb_importance.plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('XGBoost Feature Importance (Gain)', fontweight='bold')\n",
    "axes[0].set_xlabel('Importance')\n",
    "\n",
    "# Random Forest\n",
    "rf_model = results['Random Forest']['model']\n",
    "rf_importance = pd.Series(rf_model.feature_importances_, index=feature_cols).sort_values(ascending=True)\n",
    "rf_importance.plot(kind='barh', ax=axes[1], color='forestgreen')\n",
    "axes[1].set_title('Random Forest Feature Importance', fontweight='bold')\n",
    "axes[1].set_xlabel('Importance')\n",
    "\n",
    "plt.suptitle('Feature Importance Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Time-Series Cross-Validation\n",
    "\n",
    "Validate that the model performs consistently across different time periods using expanding window cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Cross-Validation on training data\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv_scores = []\n",
    "\n",
    "print('Time-Series Cross-Validation (XGBoost):')\n",
    "print('-' * 50)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):\n",
    "    X_cv_train = X_train.iloc[train_idx]\n",
    "    y_cv_train = y_train.iloc[train_idx]\n",
    "    X_cv_val = X_train.iloc[val_idx]\n",
    "    y_cv_val = y_train.iloc[val_idx]\n",
    "    \n",
    "    cv_model = XGBClassifier(\n",
    "        n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "        scale_pos_weight=(y_cv_train == 0).sum() / max((y_cv_train == 1).sum(), 1),\n",
    "        min_child_weight=10, subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=42, eval_metric='logloss', verbosity=0\n",
    "    )\n",
    "    cv_model.fit(X_cv_train, y_cv_train)\n",
    "    \n",
    "    y_cv_proba = cv_model.predict_proba(X_cv_val)[:, 1]\n",
    "    auc = roc_auc_score(y_cv_val, y_cv_proba)\n",
    "    cv_scores.append(auc)\n",
    "    \n",
    "    train_dates = train.iloc[train_idx]['date']\n",
    "    val_dates = train.iloc[val_idx]['date']\n",
    "    print(f'  Fold {fold+1}: AUC={auc:.4f}  '\n",
    "          f'(train: {train_dates.min().date()} to {train_dates.max().date()}, '\n",
    "          f'val: {val_dates.min().date()} to {val_dates.max().date()})')\n",
    "\n",
    "print(f'\\nMean AUC: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Save Best Model for Production\n",
    "\n",
    "Export the best performing model, the scaler, and feature list so the live application can use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "best_name = max(results, key=lambda k: results[k]['auc_roc'])\n",
    "best_model = results[best_name]['model']\n",
    "best_auc = results[best_name]['auc_roc']\n",
    "\n",
    "print(f'Best model: {best_name} (AUC-ROC: {best_auc:.4f})')\n",
    "\n",
    "# Save artifacts\n",
    "MODEL_DIR = os.path.join('..', 'backend', 'models')\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(MODEL_DIR, 'risk_classifier.joblib')\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f'Model saved: {model_path}')\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = os.path.join(MODEL_DIR, 'feature_scaler.joblib')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f'Scaler saved: {scaler_path}')\n",
    "\n",
    "# Save feature list and metadata\n",
    "metadata = {\n",
    "    'model_name': best_name,\n",
    "    'auc_roc': best_auc,\n",
    "    'features': feature_cols,\n",
    "    'train_date_range': [str(train.date.min().date()), str(train.date.max().date())],\n",
    "    'test_date_range': [str(test.date.min().date()), str(test.date.max().date())],\n",
    "    'test_metrics': {k: v for k, v in results[best_name].items() if k not in ['model', 'y_pred_proba', 'y_pred']},\n",
    "    'drawdown_threshold': DRAWDOWN_THRESHOLD,\n",
    "    'forward_days': FORWARD_DAYS,\n",
    "    'trained_at': datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "import json\n",
    "meta_path = os.path.join(MODEL_DIR, 'model_metadata.json')\n",
    "with open(meta_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f'Metadata saved: {meta_path}')\n",
    "\n",
    "print(f'\\nAll artifacts saved to {MODEL_DIR}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification: load and test saved model\n",
    "loaded_model = joblib.load(model_path)\n",
    "test_pred = loaded_model.predict_proba(X_test.head(5))[:, 1]\n",
    "print('Loaded model test predictions:')\n",
    "for sym, prob in zip(test[feature_cols.index].head(5) if hasattr(test, feature_cols) else range(5), test_pred):\n",
    "    print(f'  Risk probability: {prob:.4f}')\n",
    "print('\\nModel load verification successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Per-Stock Risk Scores (Live Scoring Preview)\n",
    "\n",
    "Demonstrate how the model would score each stock using the most recent data — this is what the live application will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest features for each stock\n",
    "latest_features = model_data.sort_values('date').groupby('symbol').last()[feature_cols]\n",
    "\n",
    "# Score with the best model\n",
    "if 'Logistic' in best_name:\n",
    "    latest_scaled = scaler.transform(latest_features)\n",
    "    risk_probs = best_model.predict_proba(latest_scaled)[:, 1]\n",
    "else:\n",
    "    risk_probs = best_model.predict_proba(latest_features)[:, 1]\n",
    "\n",
    "risk_df = pd.DataFrame({\n",
    "    'symbol': latest_features.index,\n",
    "    'risk_probability': risk_probs,\n",
    "}).sort_values('risk_probability', ascending=False)\n",
    "\n",
    "risk_df['risk_level'] = pd.cut(risk_df['risk_probability'],\n",
    "                                bins=[0, 0.3, 0.6, 1.0],\n",
    "                                labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "colors = risk_df['risk_level'].map({'High': 'indianred', 'Medium': 'orange', 'Low': 'forestgreen'})\n",
    "ax.barh(risk_df['symbol'], risk_df['risk_probability'], color=colors)\n",
    "ax.set_xlabel('Risk Probability (P(drawdown > 10% in 30d))', fontsize=12)\n",
    "ax.set_title('ML-Based Risk Scores — Current Portfolio', fontsize=16, fontweight='bold')\n",
    "ax.axvline(x=0.3, color='orange', linestyle='--', alpha=0.5, label='Medium threshold')\n",
    "ax.axvline(x=0.6, color='red', linestyle='--', alpha=0.5, label='High threshold')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('current_risk_scores.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nRisk Summary:')\n",
    "print(risk_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Results\n",
    "- **Best Model:** XGBoost (expected) with AUC-ROC and time-series cross-validation\n",
    "- **Features:** 18 technical features capturing volatility, momentum, RSI, MACD, Bollinger Bands, volume, drawdown, beta\n",
    "- **Temporal validation:** No data leakage — trained on 2021-2024, tested on 2025\n",
    "- **Class imbalance:** Handled via `scale_pos_weight` (XGBoost) and `class_weight='balanced'` (LogReg, RF)\n",
    "\n",
    "### Next Steps\n",
    "- **Phase 2:** SHAP explainability — understand why each stock gets its score\n",
    "- **Phase 3:** Volatility forecasting with GARCH/LSTM\n",
    "- **Integration:** Replace manual risk formula in live app with this ML model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
